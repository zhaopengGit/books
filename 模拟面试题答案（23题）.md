##### √  1. zookeeper中有那些类型的节点, 各个类型有什么特点 

```properties
1.永久节点:不依赖会话 可以创建子节点 不能同名
2.临时节点: 依赖会话 不能创建子节点 也不能同名
3.永久节点序列化:不依赖会话 可以创建子节点 同名节点会在后面添加上序号
4.临时节点序列化:依赖会话 不能创建子节点 同名节点会在后面添加上序号
```

##### √ 2. zookeeper中如何进行选举 

```properties
选举过程中每个服务器都会把票投给能力最大的,而能力是根据服务器id简称sid和事务id决定的

假设一个Zookeeper集群中有5台服务器，服务器id从1到5编号，并且它们都是最新启动的都没有历史数据,事务id为空
假设服务器依次启动:

服务器1启动,发起一次选举,服务器1一票,票数不过半,选举无效,服务器1状态保持为LOOKING
服务器2启动,发起一次选举,因为服务器2的sid比服务器1大,服务器2两票,但票数依然不过半,选举无效,服务器1,服务器2状态保持为LOOKING
服务器3启动,发起一次选举,服务器3的sid最大,服务器3三票,选举生效,服务器3称为Leader.服务器1,服务器2的状态变为为FOLLOWING
服务器4启动,此时已经选出Leader,不需要再次选举,服务器4的状态变为为FOLLOWING
服务器5同理,服务器4的状态变为为FOLLOWING

假设服务器3宕机了,1,2,4,5需要选出新的Leader,发起一次选举,这时候事务id为能力的评判标准,假设5的事务id最大,则会在新的选举中四票胜出称为新的Leader
```

##### √ 3. zookeeper中watcher机制

```properties
ZooKeeper:允许客户端向服务端注册一个 Watcher 监听，当服务端的一些事件触发了这个 Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能

zookeeper中Watch监听机制特性:一次性，客户端串行执行，轻量
```

##### √ 5. hadoop的架构, 各个架构主要有那些节点构成, 以及每个节点有什么作用

```properties
HDFS : 
NameNode:集群当中的主节点，管理元数据，主要用于管理集群当中的各种数据
SecondaryNameNode:主要能用于hadoop当中元数据信息的辅助管理
DataNode:集群当中的从节点，主要用于存储集群当中的各种数据

MapReduce:

Yarn:
ResourceManager:接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分
NodeManagers:负责执行主节点ResourceManager分配的任务
```

##### √ 6. hdfs的三个机制: 心跳机制 负载均衡机制 副本机制

```properties
心跳机制：
datanode每3秒要发送一个心跳数据包给namenode，告诉namenode还自己活着,10分钟+30秒,如果没有收到某一个datanode的心跳,则认为该节点不可用.kill掉,换新的datanode
datanode每6小时会与namenode进行一次通信，上报所有的块信息,

负载均衡机制:
namenode会定期检查集群的负载,如果发现集群中datanode节点的负载不均衡的情况,自动启动负载均衡进行负载均衡.

副本机制:
HDFS上的文件对应的 Block保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份副本。
第一副本:放置在上传文件的 DataNode上;
第二副本:放置在与第一个副本不同的机架的节点上;
第三副本:与第二个副本相同机架的不同节点上。
如果还有更多的副本:随机放在节点中。
```

#####  √ 7. hdfs的读写数据的流程

```properties
读:
1）客户端向namenode请求读取文件，namenode通过查询元数据，找到文件块所在的datanode地址。
2）客户端就近原则挑选datanode服务器，请求读取数据。
3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。
4）客户端以packet为单位接收，先在本地缓存，然后根据文件块信息拼接为要读取的文件。

写:
1）客户端向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。
2）namenode返回是否可以上传。
3）客户端请求第一个 block上传到哪几个datanode服务器上。
4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。
5）客户端请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。
6）dn1、dn2、dn3逐级应答客户端
7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答
8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器，重复以上过程，直至所有块上传完毕。
```

#####  √ 8. hdfs的seconderyNameNode辅助管理元数据的流程

```properties
第一阶段：namenode启动
（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
（2）客户端对元数据进行增删改的请求
（3）namenode记录操作日志，更新滚动日志。
（4）namenode在内存中对数据进行增删改查
第二阶段：Secondary NameNode工作
（1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。
（2）Secondary NameNode请求执行checkpoint。
（3）namenode滚动正在写的edits日志
（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode
（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。
（6）生成新的镜像文件fsimage.chkpoint
（7）拷贝fsimage.chkpoint到namenode
（8）namenode将fsimage.chkpoint重新命名成fsimage
```

##### √ 9. MR的核心思想: 分而治之的思想

```properties
就是把一个复杂的算法问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的解，把各部分的解组成整个问题的解
```

##### √ 10. MR执行流程: mapTask执行流程  reduceTask执行流程  包含shuffle过程

```properties
mapTask执行流程:
   1. 首先由map来读取数据，将读取到的数据转换成k1,v1,k1存的是每一行的起始偏移量,v1是放每一行的数据,其中maptask的数量主要取决于切片的block块数量.
   2. 接来下map根据具体业务要求将整个k1,v1转换成k2,v2.接下来会进入shuffle阶段,主要根据K2的哈希取模的值进行分区,从而得到对应分区的编号.
   3. 然后进入环形缓冲区,当欢迎缓冲区达到80%的时候,就会产生溢写的操作.这时候会对溢写的数据按照K2进行排序,如果这时候有提前聚合或者规约工作,也会在这个时候执行.
   4. 最后写入磁盘上,形成一个小文件.不断循环这个过程到整个数据读取完成.最终将所有小文件合并成一个大文件,这时候也需要对数据进行排序规约操作.


ReduceTask工作机制:
   1. Reducetask拉取属于自己的分区的数据,会先写入内存,当内存写满后,再溢写到磁盘,最后合并成大文件,合并过程中也对数据进行排序操作.
   2. 接下来会进行分组操作,整个内部已经按照K2排好序了,把相同的K2的vlaue值聚合,形成一个列表,分一次调用一次reduce方法,直到所有数据处理完成.
```

##### √ 11.Yarn提交MR的流程

```properties
1. 客户端提交任务到Yarn集群中,根据AppMaster资源要求,Yarn集群会从各个nodemanager节点中, 随机找到一台有资源的节点, 用于启动AppMaster程序, 启动后, AppMaster汇报给resourcemanager,并建立心跳 

2. resourcemanager根据要申请资源, 准备好相关的资源信息, 等待appMaster的拉取

3. AppMaster程序获取到了相关的资源后, 连接对应nodemanager节点, 让其启动MapTask、ReduceTask, 并占用相关的资源 同时,反向注册回AppMaster。

4. 接下来AppMaster开始进行任务分配工作，各个任务向AppMaster汇报自己的状态和进度，以便当任务失败时可以重启任务    
```

##### √ 12. yarn的三种调度方案各是什么, 以及有什么特征

```properties
1. 先进先出:根据作业的提交顺序，先来先服务
2. 容量调度:将资源分成多分，不同计算量的计算任务使用不同的资源;  可以根据实现资源情况动态调整资源空间大小
3. 公平调度:在计算过程中，可以将一部分空闲资源交给其他计算任务使用
```

##### √ 13. 数仓仓库的作用

```properties
1、提供加强的商业智能BI
2、提高效率和节省成本
3、提高数据的质量和一致性
4、提供历史的智慧
```

##### √ 14. 数据仓库的分层架构: 最外面的分层 和 细化分层

```properties
ODS层:贴源层
DM层：数据仓库层
	DWD层:明细数据层
	DWB层:基础数据层
	DWS层:业务层
	DM层:数据集市层
DA/RPT层:数据应用/展示层
```

##### √ 15. hive的有几种表类型, 各有什么特点

```properties
内部表:当删除内部表时候,所有数据都会删除,表创建默认创建的是内部表.可以保证数据安全性.表对自己来说有绝对控制权的时候,可以建成内部表
外部表:而删除外部表,只会删除元数据,不删除hdfs上的文件数据，在多人同时操作数据库的时候,这个时候可以建成外部表

分区表:通过分区把不同类型的数据放到不同目录中，分区的标准就是指定分区字段,分区的意义在于优化查询,可以避免查询数据时全表遍历  
分桶表:根据哈希值取余计算,将余数相同的数据保存在同一文件夹下,数据采样划分为多个文件,哈希值提升join的效率:join优化要基于分桶表
```

##### √ 16. hive的分析的函数以及 行转列 列转行   

```properties
列转行:
	EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。
    LATERAL VIEW
    用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias
    解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可                 以对拆分后的数据进行聚合。
行转列:
   concat(string,string……)：字符串连接
   concat_ws(参数1，string,string……)：参数1是分隔符,按分隔符分隔连接字符串
   collect_set(字段名)：将字段的值去重，产生array类型字段
列转行：
   Explode(字段名)：将某列中的array或map拆分成多行
   Split(字段名,分隔符)：根据分隔符来切分某字段元素
   Lateral view：写在split,explode等UDTF前，将一列数据拆成多行，再聚合
```

##### √ 17. hive的各种优化措施(结合 hive第三天 以及 项目中相关HIVE优化点)

```properties
group by造成数据倾斜:
	方案一:  基于MR的 combiner(规约, 提前聚合) 减少数据达到reduce数量, 从而减轻倾斜问题
	方案二:  负载均衡的解决方案(需要运行两个MR来处理)  (大combiner方案)
	
 HIVE 索引:
 		hive的row group index (行组索引)条件:
		1) 要求表的存储类型必须为ORC存储格式
		2) 在创建表的时候, 必须开启 row group index 索引支持
			’orc.create.index’=’true’
		3) 在插入数据的时候, 必须保证需要进行索引列, 按序插入操作
		4) 主要针对的数值类型的:bloom filter index (布隆过滤索引, 开发过滤索引):
		
	条件:
		1) 要求表的存储类型必须为ORC存储格式
		2) 在建表的时候, 必须设置为那些列构建布隆索引
		3) 仅能适用于 等值过滤查询操作
首先我们的join优化,我的了解大概有三种join优化,平时在使用join的时候,hive默认的join是reduce的join方案,整个join操作通过map传输到reduce,在reduce进行合并操作,在这种状态我们也可以看到存在有一些问题,问题主要有两类,第一个因为整个reduce数量比较少,压力会比较大,压力一大,执行效率就会比较低,从而影响整个运行 第二就是可能会造成数据倾斜,在正常情况下join的时候,会把相同的值发给同一个reduce,但其中一个数据量的值比较大,从而会造成数据倾斜.所以在这里就用到了一些join的优化,主要有三种,分别有Map Join Bucket Map Join SMB Join

表连接数据倾斜:
			（1）运行时优化:set hive.optimize.skewjoin=true; set hive.skewjoin.key=100000;默认值100000。(如果大表和大表进行join操作，则可采用skewjoin(倾斜关联)来开启对倾斜数据的优化。)
			（2）编译时优化:set hive.optimize.skewjoin.compiletime=true;
			（3）Union优化:set hive.optimize.union.remove=true;
```



##### √ 18. 维度的建模中主要有那些表模型, 各有什么特点

```properties
事实表与维度表

事实表:就是你要关注的内容；
维度表:就是你观察该事务的角度，是从哪个思维角度去观察这个内容的。
例如，某地区商品的销量，是从地区这个角度观察商品销量的。事实表就是销量表，维度表就是地区表。
```

##### √ 19. 维度建模中三种数据模型 每个模型的特点

```properties
星型模型:中心是事实表，四周是维度表.每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键。以事实 		   表为核心，维表围绕核心呈星形分布。
雪花模型:在上述星形模型基础上，每个维度表，可能又需要关联【多】个子维度表。
星座模型:当多个星型模型都有同样的维度时，可以共享同一个【维度表】
```

##### √ 20. 如何处理历史变化的数据: 缓慢渐变维

```properties
第一种：SCD1: 新行直接覆盖旧行, 仅适合于错误数据处理工作
第二种：SCD2(又称为拉链表) :它的优点是即满足了反应数据的历史状态，又能在最大程度上节省存储
第三种：SCD3:对表新增一列, 用于记录变更数据即可
	   SCD3缺点:无法维护太多历史版本(3、 5个左右), 实现较为繁琐, 不利于维护
```

##### √ 21. 如何实现拉链表流程

```properties
拉链表的实现需要在原始字段基础上增加两个新字段：
	start_time(表示该条记录的生命周期开始时间——周期快照时的状态)
	end_time(该条记录的生命周期结束时间)
	
拉链表实现步骤：
1. 建立增量数据临时表update；
2. 抽取昨日增量数据到update表；
3. 建立临时合并表tmp；
4. 合并昨日增量数据与历史数据，将重复的旧数据end_time更新为昨日，也就是从今天起不再生效；新数据end_time	改为’9999-12-31’，也就是当前有效；合并后的数据写入到tmp表；
5. 将临时表的数据，覆盖到拉链表中；
6. 下次抽取需要重建update表和tmp表。
   查询拉链表数据时，可以通过start_time和end_time查询出快照数据。
```

##### √ 22. hadoop 高可用 是如何实现的:  和zookeeper之间的原理

```properties
一主一备和一主多备:
   主服务只有一个
   主服务对外提供业务功能，一旦主服务不可用，就需要启动备用服务成为主服务对外提供功能
   
hadoop中实现服务的高可用
  HDFS:
     namenode 需要一主一备两个NN服务
     两个namenode 服务都可以对多个datanode进行管理
  Yarn:
     resourcemanager 一主一备两个服务
     两个resourcemanager 服务都可以对多个nodemanage进行管理
     
zkfc:用来监控NN的状态信息,主备NN的切换
```

##### √  23.项目

```properties
1: 请简单介绍一下你最近做的这个项目? (背景, 架构, 主要负责的点)  (5分钟)
		
	面试官您好,我最近做了小盒淘数仓分析系统项目,这是公司接的一个外包项目,项目主要是为智能零售服务商提供大数据技术支持,为企业建立数据仓库.	
	我们的整个项目架构是基于cloudera manager对大数据中各个组件统一管理,所构建的大数据分析平台,在此平台之上我们搭建有 hadoop hive sqoop hue oozie zookeeper 等大数据的相关的组件, 在这些组件当中呢，我们首先是通过Sqoop将原始数据从Mysql以及Oracle的数据导入到Hadoop的HDFS中, 通过hive进行映射,映射成功后就有了ODS层的数据,后续我们基于HIVE构建的数仓体系,去完成我们整个数仓的建设.我们基于MapReduce完成统计分析的操作, 最后将统计分析的结果通过Sqoop导出到Mysql. 最终是通过finebi完成图表展示, 整个项目是统一基于oozie完成定时调度操作。
	我负责了销售模块的构建工作,涉及到了ODS层,DWD层,DWB层,
	在ODS层数据采集的时候,我会通过Sqoop将原始数据从Mysql和oracle导入到ODS层,与原始表保持相同粒度.在ODS层也会根据业务需求去构建分区表和分桶表.为DWD层提前做准备.
	在DWD层做了一些数据清洗转换操作,将标记删除的或者为空的清洗掉,将部分数据需求进行转换,比如将年月日字段转换成年字段,月字段,日字段.也会在DWD层构建拉链表.
	在DWB层做了降维操作,先是标记好哪张是事实表,哪张是维度表.再将9张表进行left join操作,合并成一张宽表.
	
	
	我参与了前两期的项目，一共可分成两个阶段，
	第一个阶段的开发用了三个月时间，当时团队一共六个人，一名技术经理，两名数据分析师，包括我在内的剩下三名大数据开发工程师。初期构建只用了十台机子，一开始主要任务是完成基础大数据平台构建，在此基础上完成了调度平台的构建工作，各种环境的配置，在此基础之上我们完成了基础数据的迁移导入，完成了销售主题整个建模分析的操作。满足了业务方最为基础销售分析需求.
	第二个阶段开发花了四个月时间，团队新加入了一名大数据开发工程师和一名数据分析师，团队新增了到了八个人。这个阶段主要是完成集群的扩容,新增了32台机子,完成了所有数据的抽取迁移工作，同时也完成了用户、商品等模块建设工作。满足了公司80%以上的数据需求和报表需求。
	
-----------------------------------------------------------------------------        		
2: 将所负责的点, 从最开始的数据采集到最终的结果产生, 描述出整个详细项目实施流程 (30分钟)

 在做上一个项目的时候,我负责了销售模块的构建工作,一共有到9个表，
 整个过程涉及到了ODS -> DWD -> DWB。
ODS：
 1.在构建ODS层之前，我会先去了解业务需求，再此基础上，再进行建模分析，在本项目全部构建内部表；
 2.ORC文件格式可以提高hive读、写和处理数据的能力，所以项目采用了ORC文件格式，项目前期我们的硬件资源不是很足，考虑到ODS层读取次数较少，所有我选择了ZLIB压缩方案,优先保证压缩率.
 3.根据业务需求，部分表根据日期字段构建成了分区表,部分表数据量比较大的,后期需要进行join优化的表,构建成分桶表。
 4.建模好了后,在数据的采集的时候，我会通过Sqoop工具将Mysql和Oracle中的原始数据同步到ODS层,保持和原始数据一样的粒度
 5.根据数据表的属性,确认同步方式:
 数据量并不是很大，而且不需要维护历史变化行为，会选择全量覆盖同步方式,1张
 数据量比较庞大且没有更新数据的操作，只新增数据，会选择仅新增同步方式,6张
 数据量比较大, 而且数据还存在历史变更情况,会选择新增和更新同步方式,2张
 在同步数据时需要把之前的表删掉，主要是因为HCatalog不允许数据覆盖，每次都只是追加.再进行全量和增量的导入
  
DWD：
	在构建DWD层的时候,会根据业务需求,要对一些表进行数据清洗、转换等操作还有一些拉链表前期会有一些准备工作。
	在建模的时候会和ODS层保持相同粒度，压缩方案会用SNAPPY。数据同步的时候，根据实际情况需要，通过if判断去清洗掉标记删除的数据、空为null的数据；对那些标记了数据转换的,比如原有用 0 1表示安卓 苹果，我们直接将其转换为 安卓 苹果；项目要求日期粒度要到日，所以我进行扩宽操作，把原有一个字段中含有年月日, 拆解为年字段 月字段 和 日字段；
	再然后就是做拉链表，需要在原始字段基础上增加两个新字段：start_time 开始时间，end_time 结束时间.
	拉链表实现步骤：1. 建立增量数据临时表update；2. 抽取昨日增量数据到update表；3. 建立临时合并表tmp；4. 合并昨日增量数据与历史数据，再通过if判断，将重复的旧数据end_time更新为昨日，也就是从今天起不再生效；新数据end_time改为’9999-12-31’，也就是当前有效；通过union   all合并后的数据写入到tmp表；5. 将临时表的数据，覆盖到拉链表中；6. 下次抽取需要重建update表和tmp表。查询拉链表数据时，可以通过start_time和end_time查询出快照数据。

DWB：
DWB层我主要是要构建一个销售模块上的9个表汇总成一个宽表,我先找出哪些表上事实表，那些表是维度表，确认好事实表后，开始建模，保留事实表的所有字段，将维度表中需要的字段提取出来，不需要的字段视情况删减。导入数据的时，先是找到表与表之间的对应关系，再进行left join操作，再次过程中通过开启一系列的map join优化服务，设置相关参数，解决发生的数据倾斜问题。 

整体都完成后，我会写个shell脚本，然后用oozie去做定时调度，固定每天凌晨3点进行。
	
------------------------------------------------------------------------------

3: 在整个开发中 有没有遇到过一些问题, 如何发现的, 如何解决的? 千万不要讲一些影响你能力问题, 能够展示你能力的问题，比如跟优化相关的问题或者跟新技术相关问题  (5分钟)

发现问题:
	 当hive sql运行的比预期的慢很多 或者 任务进度条一直停留在了99%,我会怀疑发生了数据倾斜,这时候我会去yarn去查看reduce的历史记录, 看到某一个reduce的记录数与平均记录数大三倍以上，最长时长也远大于平均时长,数据量分配极度不均衡的现象,就能确定发生了数据倾斜
解决问题:
     解决数据倾斜首先我会检查的我sql语句本身是否就有数据倾斜,尝试优化后,效果不明显, 我就根据表的结构,表的数据量,选择一个合适的join优化方案,假设最终我选择了map join,我会在hive中开启map join服务,设置小表阈值, 让小表数据在map task的内存中与大表数据进行匹配,合并,这有利于减少reduce的工作量,从而达到了优化的效果,从而也可用最终节省了大量的时间. 如果是group by导致的数据倾斜,也是根据实际情况开启相关参数,就能解决数据倾斜问题. 因为涉及到等值过滤查询操作,所以我也会开启布隆过索引.

------------------------------------------------------------------------------

4- 项目的真实情况问题(同上选答)
```

 

